# 概率论基本概念

## 充分统计量

表示一个浓缩了分布中的所有参数信息的函数，即描述整个数据集，当知道充分统计量后，原始数据与分布的参数$\theta$ 条件独立，即数据x不再能提供任何参数的信息

在已知充分统计量的前提下可以得到一个分解式
$$
p(\theta|X) = p(\theta|T(X)) \quad\quad\quad(1)
$$

或者
$$
p(X|\theta) = g(T(X), \theta)h(X)\quad\quad\quad(2)
$$
其中(1)式理解为参数的分布已与X无关，(2)式中，将X的概率分布分解为两个部分，一个部分是包含了T(X)的函数，即包含了所有与参数相关的函数，与X相关的h(X) 与参数无关，也就是X不会对分布$\theta$ 有任何影响

## 似然估计

**最大似然函数**（Maximum Likelihood Function）是统计学中用于估计模型参数的一种方法。其核心思想是：给定一个数据集，找到一组参数使得在这些参数下，观察到该数据集的可能性（或概率）最大。

### 1. **最大似然估计 (MLE)** 的基本原理：

假设你有一组独立同分布的数据 $\mathbf{X} = \{x_1, x_2, \dots, x_n\}$，这些数据来自一个带有参数 $\theta$ 的概率分布 $p(x|\theta)$。最大似然估计的目标是找到一组参数 $\theta$ ，使得给定数据的情况下，这个参数能够最大化观测数据的联合概率（或称为似然）。

### 2. **似然函数**：
似然函数 $L(\theta|\mathbf{X})$ 定义为给定参数 $\theta$ 下，数据集 $\mathbf{X}$ 的联合概率。对于独立同分布的数据，似然函数可以表示为所有数据点概率的乘积：

$$
L(\theta|\mathbf{X}) = \prod_{i=1}^{n} p(x_i|\theta)
$$

由于乘积形式的计算复杂，通常取对数，得到**对数似然函数**（log-likelihood function）：

$$
\log L(\theta|\mathbf{X}) = \sum_{i=1}^{n} \log p(x_i|\theta)
$$

### 3. **最大化对数似然**：
最大似然估计的任务就是找到参数 $\theta$，使得对数似然函数达到最大值。这通常通过优化技术来实现，比如梯度上升（或下降）。找到的参数 $\hat{\theta}$ 就是**最大似然估计量**。

$$
\hat{\theta} = \arg\max_{\theta} \log L(\theta|\mathbf{X})
$$

### 4. **最大似然估计的直观理解**：
可以这样理解：假设我们有一组数据，来自一个我们不了解参数 $\theta$ 的分布。我们希望通过最大似然的方法，找到一组参数，使得在这些参数下，观察到我们这组数据的可能性最大。换句话说，我们通过这种方法去“反推”哪个参数最有可能生成这组数据。

### 5. **举个例子**：
假设我们有一个伯努利分布（只有0和1的二项分布），参数 $\theta$ 是成功的概率。假设观察到了一组数据 $\mathbf{X} = \{1, 0, 1, 1\}$。最大似然估计的任务是找到 $\theta$ ，使得 $\mathbf{X}$出现的可能性最大。

似然函数是：

$$
L(\theta|\mathbf{X}) = \theta^3(1-\theta)^1
$$

取对数：

$$
\log L(\theta|\mathbf{X}) = 3\log(\theta) + \log(1-\theta)
$$

然后，我们对$\theta$ 求导并找到极值点，得到最大似然估计 $\hat{\theta}$，这就是最符合观测数据的参数。

### 6. **最大似然估计的性质**：
- **渐进无偏性**：随着数据量的增加，最大似然估计量 $\hat{\theta}$ 会逐渐逼近真实的参数值。
- **渐进正态性**：在大样本下，最大似然估计量的分布趋于正态分布。

总结来说，最大似然估计是一种通过最大化观测数据的概率来推断参数的方法。它在机器学习、统计建模和数据分析中被广泛使用。

# Some Matrix

## doubly stochastic 双随机矩阵

**Doubly Stochastic Matrix（双随机矩阵）** 是线性代数中的一个特殊矩阵，其特点是每一行和每一列的元素都满足概率分布的性质，即所有元素非负，并且每一行和每一列的元素之和都为1。

具体来说，如果矩阵 $\mathbf{M}$ 是 $n \times n$ 的双随机矩阵，则满足以下两个条件：

1. **非负性**：矩阵中的每个元素 $M_{ij} \geq 0$ 对所有 $i$ 和 $j$ 都成立。
2. **行和为1**：矩阵每一行的元素之和为1，即：
   $$
   \sum_{j=1}^{n} M_{ij} = 1, \quad \forall i
   $$
3. **列和为1**：矩阵每一列的元素之和为1，即：
   $$
   \sum_{i=1}^{n} M_{ij} = 1, \quad \forall j
   $$

换句话说，双随机矩阵不仅在行的方向上满足概率分布的性质（行向量的元素之和为1），而且在列的方向上也满足相同的性质（列向量的元素之和为1）。

### 示例
例如，下面是一个 $3 \times 3$ 的双随机矩阵：

$$
\mathbf{M} = \begin{pmatrix}
0.3 & 0.3 & 0.4 \\
0.2 & 0.5 & 0.3 \\
0.5 & 0.2 & 0.3
\end{pmatrix}
$$

- 每行的元素之和都是1：$0.3 + 0.3 + 0.4 = 1$，$0.2 + 0.5 + 0.3 = 1$，$0.5 + 0.2 + 0.3 = 1$。
- 每列的元素之和也是1：$0.3 + 0.2 + 0.5 = 1$，$0.3 + 0.5 + 0.2 = 1$，$0.4 + 0.3 + 0.3 = 1$。

### 应用
双随机矩阵在许多领域都有应用，特别是在概率和优化相关的领域中，例如：

1. **马尔可夫链**：双随机矩阵可以作为马尔可夫链的转移矩阵，用于描述系统状态间的转移概率。
2. **图论**：在图的拉普拉斯矩阵中，双随机矩阵被用于描述某些正则图（regular graphs）中的顶点邻接关系。
3. **Optimal Transport（最优传输）**：最优传输问题中经常需要寻找双随机矩阵来最小化传输成本。

双随机矩阵的性质使它成为概率和图分割等领域中的重要工具。

## 亲和矩阵

**亲和矩阵（Affinity Matrix）** 是表示数据点之间相似性的一种矩阵形式，广泛应用于机器学习、图论、谱聚类和图像处理等领域。每个矩阵元素描述了两个数据点之间的**相似性或亲和度**，值越大表示数据点之间的相似性越高。

### 1. **定义**
亲和矩阵是一个对称矩阵，设 $A$ 是一个 $N \times N$ 的亲和矩阵，$N$ 是数据点的数量，那么矩阵元素 $A_{ij}$ 表示数据点 $x_i$ 和 $x_j$ 之间的亲和度（相似性）。常见的亲和矩阵通常基于距离度量或相似度度量构建。

- **矩阵维度**：亲和矩阵是方阵，大小为 $N \times N$。
- **矩阵元素**：$A_{ij}$ 代表数据点 $x_i$ 和 $x_j$ 之间的相似性，通常使用某种相似度函数计算。

### 2. **亲和矩阵的构建**
构建亲和矩阵的核心是如何定义相似性，常见的相似度计算方式有：

#### (1) **高斯核函数（Gaussian Kernel）**
高斯核是构建亲和矩阵的一种常用方法，通常用于处理数值型数据，定义如下：
$$
A_{ij} = \exp\left( -\frac{\|x_i - x_j\|^2}{2\sigma^2} \right)
$$
其中：
- $\|x_i - x_j\|$ 是数据点 $x_i$ 和 $x_j$ 之间的欧几里得距离。
- $\sigma$ 控制相似性的范围，值越小则表示相似性衰减得越快。

高斯核函数的特点是当两个点的距离较近时，它们的亲和度较高，距离较远时亲和度迅速下降。

#### (2) **余弦相似度（Cosine Similarity）**
余弦相似度用于衡量两个向量之间的夹角，特别适合高维稀疏数据（如文本向量）。定义为：
$$
A_{ij} = \frac{x_i \cdot x_j}{\|x_i\| \|x_j\|}
$$
其中：
- $x_i \cdot x_j$ 是两个向量的点积。
- $\|x_i\|$ 和 $\|x_j\|$ 是向量的范数。

余弦相似度的值在 -1 到 1 之间，值越大表示两个向量的方向越接近，亲和度越高。

#### (3) **$k$-近邻（$k$-Nearest Neighbor, $k$-NN）**
在 $k$-近邻方法中，亲和矩阵可以基于数据点的 $k$-近邻来构建：
- 如果数据点 $x_i$ 是数据点 $x_j$ 的 $k$-近邻，则 $A_{ij} = 1$。
- 如果不是 $k$-近邻，则 $A_{ij} = 0$。

这种方法构建的亲和矩阵稀疏，适合大规模数据集。

### 3. **亲和矩阵的性质**
- **对称性**：亲和矩阵通常是对称的，即 $A_{ij} = A_{ji}$，因为大多数相似性度量是对称的。
- **非负性**：矩阵元素一般为非负值，因为相似性或亲和度通常用正数表示。
- **稀疏性**：在某些应用中，尤其是使用 $k$-近邻方法时，亲和矩阵通常是稀疏的，即大部分元素为零，这有利于计算效率的提升。

### 4. **应用**
亲和矩阵在机器学习和数据分析中有广泛的应用，以下是一些典型场景：

#### (1) **谱聚类（Spectral Clustering）**
谱聚类是一种基于图的聚类算法，它利用亲和矩阵表示数据点之间的相似性，然后通过对图的拉普拉斯矩阵进行特征分解来实现聚类。亲和矩阵在谱聚类中是核心输入，用于描述数据点之间的连接关系。

#### (2) **降维**
亲和矩阵可以用于流形学习中的降维算法，如**拉普拉斯特征映射（Laplacian Eigenmaps）**和**局部线性嵌入（LLE, Locally Linear Embedding）**，这些方法通过构建亲和矩阵来保留数据点的局部结构，找到数据的低维嵌入表示。

#### (3) **图分割**
在图像分割等任务中，亲和矩阵用于描述像素或超像素之间的相似性。基于亲和矩阵，可以将相似的像素聚集在一起形成图像中的分割区域。

#### (4) **半监督学习**
在半监督学习中，亲和矩阵可以用于表示标注数据和未标注数据之间的关系，通过亲和矩阵的信息传播来推断未标注数据的标签。

### 5. **示例**
假设我们有一个包含 4 个数据点的简单二维数据集：
$$ X = \{x_1, x_2, x_3, x_4\} $$

计算它们之间的相似性，可以用欧几里得距离和高斯核构建亲和矩阵：
$$
A = \begin{bmatrix}
0 & 0.8 & 0.1 & 0.3 \\
0.8 & 0 & 0.2 & 0.5 \\
0.1 & 0.2 & 0 & 0.7 \\
0.3 & 0.5 & 0.7 & 0
\end{bmatrix}
$$

在这个矩阵中，$A_{12} = 0.8$ 表示数据点 $x_1$ 和 $x_2$ 之间的相似度较高，而 $A_{13} = 0.1$ 表示数据点 $x_1$ 和 $x_3$ 之间的相似度较低。

### 总结
- **亲和矩阵**是一种表示数据点之间相似性关系的矩阵形式，用于描述各数据点之间的连接强度。
- 亲和矩阵广泛应用于聚类、降维和图分割等任务中，能够帮助算法利用数据点之间的关系进行更好的分析和学习。
- 构建亲和矩阵的相似度度量方式多种多样，选择合适的度量方式取决于数据的特性和任务需求。

# Manifold 流形

**流形（manifold）** 和 **非线性流形（nonlinear manifold）** 是几何学和数据分析中常用的概念，特别是在处理高维数据时有重要作用。

### 1. **流形（Manifold）**
在数学中，流形是一种**局部平坦的**几何空间。直观理解，虽然流形可能在全局上具有复杂的形状，但在每个局部区域，它看起来像是一个平坦的欧几里得空间。

举个例子：
- **二维平面**就是一个简单的流形，它在局部和整体上都是平坦的。
- **球面**也是一个常见的流形，尽管球体的整体形状是弯曲的，但在局部的任何小区域（如地球表面的小区域）看起来都像是一个平面。

#### 流形的特征：
- **局部欧几里得性**：流形的每个局部区域都可以嵌入到一个低维的欧几里得空间中。比如地球表面（球面）虽然是三维空间的一部分，但它在局部看起来是二维的。
- **维度**：流形的维度表示它的局部欧几里得空间的维度。例如，二维球面是嵌入在三维空间中的二维流形。
- **可微性**：流形的每个局部区域通常具有可微性，这使得可以在流形上应用微积分等数学工具。

### 2. **非线性流形（Nonlinear Manifold）**
**非线性流形**是指那些**局部是平坦的欧几里得空间，但整体上不是线性的空间**。换句话说，非线性流形在全局上呈现出复杂、弯曲的几何形态，而不像一个直线的空间。

#### 示例：
- **球面**：球面是非线性流形的典型例子。它局部上是平坦的（看起来像一个二维平面），但整体上是一个三维空间中的曲面。
- **圆环面（Torus）**：像甜甜圈形状的圆环面，它在局部看起来是二维的平坦空间，但整体上是嵌入到三维空间中的复杂形状。

在数据分析中，很多时候数据本身位于一个**非线性流形**上。尽管数据嵌入在高维空间中（比如图像数据通常在非常高维的空间中表示），但这些数据点可能实际分布在一个比这个高维空间低得多的非线性流形上。例如，手写数字的图片虽然在像素空间中可能有几千个维度，但它们实际可以被表示为一个更低维的、嵌入到高维空间的非线性流形。

### 3. **流形与数据分析的关系**
在高维数据分析中，我们通常希望找到流形结构，因为：
- 数据在高维空间中虽然看起来是复杂和分散的，但它们可能沿着某个低维的流形进行分布。这种流形结构能帮助我们更好地理解数据的本质。
- 流形学习方法（如**流形嵌入**）可以将高维数据降维到更低的维度，同时保留其内在的几何结构。

### 4. **流形学习（Manifold Learning）**
流形学习是基于数据位于低维流形假设的降维方法。通过流形学习，算法试图找到数据在低维空间中的内在结构，常见的方法有：
- **局部线性嵌入（LLE, Locally Linear Embedding）**：假设数据在局部是线性的，通过保留局部邻居间的关系，找到数据的低维表示。
- **拉普拉斯特征映射（Laplacian Eigenmaps）**：基于图的方式，通过构建相似图来嵌入数据，保留数据的局部几何特性。
- **同类特征映射（Isomap）**：它假设数据点位于流形上，并使用地理距离代替欧几里得距离来保持数据的流形结构。

### 总结
- **流形**是一种在局部看起来像欧几里得空间，但在全局上可能具有复杂形状的几何结构。
- **非线性流形**是指那些整体上呈现出非线性形态，但在局部区域上是平坦的空间。
- 在数据分析中，很多高维数据集可以看作是嵌入在高维空间中的低维非线性流形，识别和利用这些流形结构能够帮助我们进行有效的降维和数据分析。

通过使用流形学习算法，可以捕捉数据的非线性结构，提取有意义的低维特征。

